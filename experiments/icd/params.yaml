prepare:
    MinLabelFrequency: 10
    MaxNumberLabels: -1

preprocess:
    tokenizer: Spacy
    max_length: 256
    embedding: "gsg-fasttext"

label_tree:
    group_id_chars: 0

model:
    dropout: 0.5
    encoder: 
        hidden_size: 256
        num_layers: 1
    attention: 
        type: "softmax-attention"
    mlp:
        bias: true
        activation: 'relu' 
        hidden_layers:
            - 256

trainer:
    regime: "levelwise"
    save_interval: 5000
    eval_interval: 150
    train_batch_size: 128
    eval_batch_size: 256
    num_steps: 10_000
    num_candidates: null
    topk: 1
